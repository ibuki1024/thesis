\contentsline {section}{\numberline {1}序論}{1}
\contentsline {section}{\numberline {2}はじめに}{1}
\contentsline {section}{\numberline {3}方策勾配を用いた強化学習}{1}
\contentsline {subsection}{\numberline {3.1}強化学習の基礎知識}{1}
\contentsline {subsection}{\numberline {3.2}方策反復法}{2}
\contentsline {subsection}{\numberline {3.3}状態空間, 行動空間の特性に合わせたアルゴリズム}{2}
\contentsline {subsection}{\numberline {3.4}方策勾配による方策関数のパラメータの更新}{2}
\contentsline {section}{\numberline {4}最適セルフトリガー制御問題に対する強化学習}{4}
\contentsline {subsection}{\numberline {4.1}セルフトリガー制御}{4}
\contentsline {subsection}{\numberline {4.2}最適セルフトリガー制御}{4}
\contentsline {section}{\numberline {5}数値実験}{5}
\contentsline {subsection}{\numberline {5.1}方策関数の表現モデル}{5}
\contentsline {subsection}{\numberline {5.2}初期方策}{5}
\contentsline {subsection}{\numberline {5.3}学習によって得られた方策}{6}
\contentsline {section}{\numberline {6}課題点の抽出}{7}
\contentsline {subsection}{\numberline {6.1}方策の最適性}{7}
\contentsline {subsection}{\numberline {6.2}$Q$関数の近似精度}{7}
\contentsline {subsection}{\numberline {6.3}$Q$関数の近似精度が低い理由}{8}
\contentsline {section}{\numberline {7}探索手法の工夫}{9}
\contentsline {subsection}{\numberline {7.1}探索}{9}
\contentsline {subsection}{\numberline {7.2}探索ノイズの大きさ}{10}
\contentsline {subsection}{\numberline {7.3}理想ケースとそれを達成する工夫}{10}
\contentsline {subsection}{\numberline {7.4}提案手法による実験結果と考察}{11}
\contentsline {subsection}{\numberline {7.5}提案手法の妥当性の検討}{11}
\contentsline {section}{\numberline {8}まとめ}{11}
\contentsline {section}{参考文献}{12}
\contentsline {section}{\numberline {付録A}aho}{12}
