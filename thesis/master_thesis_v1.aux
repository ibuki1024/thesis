\relax 
\citation{DQN}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background of Deep Reinforcement Learning}{1}}
\newlabel{purpose_of_rl}{{1}{1}}
\newlabel{Q_func}{{4}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Iteration}{1}}
\citation{DPG}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Algorithms adapted to the settings of state and action space}{2}}
\newlabel{sec:policy_improvement}{{2.3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Deterministic Policy Gradient Method}{2}}
\newlabel{true_pg}{{5}{2}}
\newlabel{expectation_approximation}{{9}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Exploration in DDPG}{3}}
\newlabel{sec:exploration}{{2.5}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Self-Triggered Control}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces control system}}{4}}
\newlabel{image}{{1}{4}}
\newlabel{continuous_system}{{11}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Optimal Self-Triggered Control}{4}}
\newlabel{optimal_policy}{{12}{4}}
\newlabel{value}{{14}{4}}
\newlabel{reward}{{15}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Reinforcement Learning for Optimal Self-Triggered Control}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Settings}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Agent Model}}{5}}
\newlabel{NN}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Consideration}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Linear Case}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Initial Policy}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Learned Policy}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Comparison with Naiive Model Based Control}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Non-Linear Case}{6}}
\newlabel{pendulum}{{19}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Initial Policy}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A control path with learned policy $\pi _{\textrm  {RL}}$}}{7}}
\newlabel{path_1}{{3}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Learned Policy}{7}}
\newlabel{compare_policy}{{21}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Optimality}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Approximation Accuracy of Critic}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Ingenuity}{8}}
\bibcite{Q}{1}
\bibcite{DQN}{2}
\bibcite{DPG}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Action distribution in replay buffer around $s=[0\nobreakspace  {}0]$.}}{9}}
\newlabel{data_distribution}{{4}{9}}
\newlabel{ingenuity}{{29}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{9}}
\@writefile{toc}{\contentsline {section}{References}{9}}
\bibcite{DDPG}{4}
\bibcite{off-PAC}{5}
\bibcite{approximation}{6}
\bibcite{Adam}{7}
\citation{polya1945}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This is a table.}}{10}}
