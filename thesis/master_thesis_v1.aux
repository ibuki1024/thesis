\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background of Deep Reinforcement Learning}{2}}
\newlabel{purpose_of_rl}{{1}{2}}
\newlabel{Q_func}{{4}{2}}
\citation{DQN}
\citation{DPG}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Iteration}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Algorithms adapted to the settings of state and action space}{3}}
\newlabel{sec:policy_improvement}{{2.3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Deterministic Policy Gradient Method}{3}}
\newlabel{true_pg}{{5}{3}}
\newlabel{expectation_approximation}{{9}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Exploration in DDPG}{4}}
\newlabel{sec:exploration}{{2.5}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Self-Triggered Control}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces control system}}{5}}
\newlabel{image}{{1}{5}}
\newlabel{continuous_system}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Optimal Self-Triggered Control}{6}}
\newlabel{optimal_policy}{{12}{6}}
\newlabel{value}{{14}{6}}
\newlabel{reward}{{15}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Reinforcement Learning for Optimal Self-Triggered Control}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Settings}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Consideration}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Agent Model}}{7}}
\newlabel{NN}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Linear Case}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Initial Policy}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Learned Policy}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A control path with learned policy $\pi _{\textrm  {RL}}$}}{8}}
\newlabel{path_l}{{3}{8}}
\newlabel{compare_policy}{{19}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Comparison with Naiive Model Based Control}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Non-Linear Case}{8}}
\newlabel{pendulum}{{22}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Initial Policy}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Learned Policy}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A control path with learned policy $\pi _{\textrm  {RL}}$}}{9}}
\newlabel{path_1}{{4}{9}}
\newlabel{compare_policy}{{24}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Optimality}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Approximation Accuracy of Critic}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Action distribution in replay buffer around $s=[0\nobreakspace  {}0]$.}}{11}}
\newlabel{data_distribution}{{5}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Ingenuity}{11}}
\newlabel{ingenuity}{{31}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}}
\bibcite{Q}{1}
\bibcite{DQN}{2}
\bibcite{DPG}{3}
\bibcite{DDPG}{4}
\bibcite{off-PAC}{5}
\bibcite{approximation}{6}
\bibcite{Adam}{7}
\bibcite{STC}{8}
\citation{polya1945}
\@writefile{toc}{\contentsline {section}{References}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This is a table.}}{12}}
