\relax 
\citation{DQN}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background of Deep Reinforcement Learning}{1}}
\newlabel{purpose_of_rl}{{1}{1}}
\newlabel{Q_func}{{4}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Policy Iteration}{1}}
\citation{DPG}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Algorithms adapted to the settings of state and action space}{2}}
\newlabel{sec:policy_improvement}{{2.3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Deterministic Policy Gradient Method}{2}}
\newlabel{true_pg}{{5}{2}}
\newlabel{expectation_approximation}{{9}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Self-Triggered Control}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces control system}}{3}}
\newlabel{image}{{1}{3}}
\newlabel{continuous_system}{{10}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Optimal Self-Triggered Control}{4}}
\newlabel{optimal_policy}{{11}{4}}
\newlabel{value}{{13}{4}}
\newlabel{reward}{{14}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Reinforcement Learning for Optimal Self-Triggered Control}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model Settings}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Agent Model}}{5}}
\newlabel{NN}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Consideration}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Linear Case}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Initial Policy}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Learned Policy}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Non-Linear Case}{5}}
\newlabel{pendulum}{{18}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Initial Policy}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Learned Policy}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A control path with learned policy $\pi _{\textrm  {RL}}$}}{6}}
\newlabel{control_path}{{3}{6}}
\newlabel{compare_policy}{{20}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Optimality}{6}}
\bibcite{Q}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}方策の最適性}{7}}
\newlabel{optimality}{{23}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{7}}
\@writefile{toc}{\contentsline {section}{References}{7}}
\bibcite{DQN}{2}
\bibcite{DPG}{3}
\bibcite{DDPG}{4}
\bibcite{off-PAC}{5}
\bibcite{approximation}{6}
\bibcite{Adam}{7}
\citation{polya1945}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces This is a table.}}{8}}
